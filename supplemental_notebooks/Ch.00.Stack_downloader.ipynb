{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6052db96-5cd5-4e34-8a15-c9bb1c56c3c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# From Social Science to Data Science: Preparing a Stack Exchange DataFrame\n",
    "\n",
    "**Author**: Bernie Hogan, bernie.hogan@oii.ox.ac.uk\n",
    "\n",
    "**Last Modification**: October 17, 2023\n",
    "\n",
    "Run this cell below for a one-click solution to get from any Stack Exchange archive to a raw XML file and DataFrame pickle that you can use with Chapters 10-13 of Hogan's _From Social Science to Data Science_. \n",
    "\n",
    "See bottom of file for change log. \n",
    "\n",
    "## To use\n",
    "\n",
    "Simply run the cell. The rest should be self-explanatory from the widgets (i.e. buttons) it produces. Please note that all required packages should have already been installed with anaconda, except possibly `py7zr` which should download and install automatically from `pip`. \n",
    "\n",
    "## Features\n",
    "\n",
    "This is script is very advanced compared to what you have seen in the book, but it shouldn't be completely intimidating. Compared to the code in Chapter 10, here are some interesting features that you might want to check out: \n",
    "\n",
    "- I use `ipywidgets` in order to create buttons instead of just running code. \n",
    "- I use `tqdm` to create a progress bar for the file. \n",
    "- I update the status of the processing during the work.\n",
    "- I use `ElementTree` instead of `json_normalise()`. It's more fussy, but it's faster.\n",
    "- I refactored the `cleanBody` and `cleanTags` so that it is only one call to BeautifulSoup.\n",
    "- I use `multiprocessing` in order to make the processing of text work a lot faster on modern multi-core processors. \n",
    "- I have options so that you can keep the original 7zip archive if you wish. \n",
    "- The methods should all have docstrings and type hinting. \n",
    "- ChatGPT gave lots of useful pointers and tips, but no large code snippets.\n",
    "\n",
    "## Future extensions \n",
    "\n",
    "This script hardcodes for several things which might be made more general. \n",
    "- The original archive at \"https://archive.org/download/stackexchange\"\n",
    "- The default download directory (in a sibling folder to the parent of this file under 'data')\n",
    "\n",
    "It would be useful in the future to:\n",
    "- Ensure compatibility when storing data on Google Drive for Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6760b4c8-caa9-4c03-8632-b5da443611e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c555cc16c774c8294121230a29cc0ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e42a13606b47a088190bf994637c03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Archive:', layout=Layout(width='50%'), options=('0. 3dprinting - 16.6M', â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import ipywidgets as iw\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, Markdown\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import warnings\n",
    "import os\n",
    "import time\n",
    "import bs4 \n",
    "import xml.etree.ElementTree as ET\n",
    "import multiprocessing\n",
    "\n",
    "# This might not be installed. It shouldn't cause trouble to live install\n",
    "try: \n",
    "    import py7zr\n",
    "except ModuleNotFoundError:\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install py7zr\n",
    "    import py7zr\n",
    "\n",
    "\n",
    "URL = \"https://archive.org/download/stackexchange\"\n",
    "DOWNLOAD_FOLDER = Path().cwd().parent / \"data\"\n",
    "\n",
    "def download_table(url = \"https://archive.org/download/stackexchange\"):\n",
    "    \"\"\"\n",
    "    Downloads URL and parses for tables, and returns first table, which is a directory listing of all Stack Exchanges. \n",
    "\n",
    "    Parameters: \n",
    "    url (str): A path to the directory listing.\n",
    "\n",
    "    Returns: \n",
    "    pd.DataFrame: Table as unformatted DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        result = pd.read_html(url)\n",
    "        return(result[0])\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def process_archive_table(df: pd.DataFrame) -> pd.DataFrame: \n",
    "    \"\"\"\n",
    "    Processes the text from an Internet Archive listing of Stack Exchanges.\n",
    "\n",
    "    Returns: \n",
    "    pd.DataFrame: A DataFrame with new columns for the name of the Exchange.\n",
    "             Excludes StackOverflow due to size limitations and meta.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract valid stacks from directory listing\n",
    "    df['stack_file_name'] = (\n",
    "        df.Name.str\n",
    "        .extract( r\"(?P<stack_name>\\w*\\.stackexchange\\.com.7z)\"))\n",
    "\n",
    "    # Filter to valid stacks\n",
    "    df = (\n",
    "        df[df['stack_file_name']\n",
    "        .map(lambda x: isinstance(x, str) and not \"meta\" in x)]\n",
    "        .copy())\n",
    "\n",
    "    # Create nice column\n",
    "    df['stack_name'] = (\n",
    "        df['stack_file_name']\n",
    "        .map(lambda x: x.split(\".\")[0]))\n",
    "\n",
    "    df.reset_index(inplace=True,drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def download_archive(\n",
    "    url: str, dest: str, output_widget: iw.Output, **kwargs) -> str:\n",
    "    \"\"\"\n",
    "    Function to download a Stack Exchange archive. It takes in an ipyoutput widget in order to use to update download progress.\n",
    "\n",
    "    Returns:\n",
    "    Path: A path object pointing to the downloaded file\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(dest, str):\n",
    "        dest = Path(dest)\n",
    "\n",
    "    if not dest.exists():\n",
    "        with output_widget: \n",
    "            display(Markdown(f\"Please create a folder at the location:\\n {download_folder}\\n\\nNote: You can use relative paths with `..` to mean one folder up. So if you are in a parallel folder to your data folder, `../data` (or `..\\\\data` on windows) will be what you need. Otherwise, you can always write the full path name such as:\\n{download_folder.resolve()} .\"))\n",
    "        return False\n",
    "    else:\n",
    "        DOWNLOAD_FOLDER = dest.resolve()\n",
    "\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    dest_file = dest / filename\n",
    "\n",
    "    resp = requests.get(url, stream=True)\n",
    "    total = int(resp.headers.get('content-length', 0))\n",
    "\n",
    "    if total <= 0: \n",
    "        with output_widget:\n",
    "            print('The file size was 0. Is the URL correct?')\n",
    "            return False\n",
    "\n",
    "    with output_widget: \n",
    "        with open(dest_file, 'wb') as file, tqdm(\n",
    "        total=total,\n",
    "        unit='iB',\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "        ) as bar:\n",
    "            for data in resp.iter_content(chunk_size=1024):\n",
    "                size = file.write(data)\n",
    "                bar.update(size)\n",
    "\n",
    "    if 'extract' in kwargs and kwargs['extract']:\n",
    "        if 'keep7z' in kwargs:\n",
    "            archive_folder = extract_archive(\n",
    "                dest_file,keep7z=kwargs['keep7z']) \n",
    "        return archive_folder\n",
    "    else:\n",
    "        return dest_file\n",
    "\n",
    "\n",
    "def extract_archive(archive_path: Path, **kwargs) -> Path:\n",
    "    \"\"\"\n",
    "    Function to extract a StackExchange archive from 7z.\n",
    "    The archive is placed in a folder of the same name. \n",
    "\n",
    "    Parameters:\n",
    "    keep7z (Boolean): Keep the original 7z file. It is deleted by default.\n",
    "\n",
    "    Returns:\n",
    "    str: the folder path containing the XML files.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(archive_path, str):\n",
    "        archive_path = Path(archive_path)\n",
    "\n",
    "    file_name = archive_path.name\n",
    "    folder_name = \".\".join(archive_path.name.split(\".\")[:-1])\n",
    "    archive_folder = archive_path.parent / folder_name\n",
    "    archive_folder.mkdir(exist_ok=True)\n",
    "\n",
    "    with py7zr.SevenZipFile(archive_path, 'r') as archive:\n",
    "        archive.extractall(archive_folder)\n",
    "\n",
    "    if 'keep7z' in kwargs and kwargs['keep7z']:\n",
    "        pass\n",
    "    else:\n",
    "        try:\n",
    "            os.remove(archive_path)\n",
    "        except:\n",
    "            print(\"The original 7z could not be deleted\")\n",
    "\n",
    "    return archive_folder\n",
    "\n",
    "from collections.abc import Sequence \n",
    "\n",
    "def _seq_but_not_str(obj):\n",
    "    return isinstance(obj, Sequence) and not isinstance(obj, (str, bytes, bytearray))\n",
    "\n",
    "\n",
    "def convertXMLtoDataFrame(xml_path: Path, cols = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parser to process an XML file into a DataFrame. Presently assumes it is\n",
    "    a StackExchange file with the structure of Posts.xml\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Unprocessed DataFrame directory from XML data.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(xml_path, str):\n",
    "        xml_path = Path(xml_path)\n",
    "\n",
    "    assert xml_path.exists(), \"The file path is not valid\"\n",
    "    \n",
    "    # Parse the XML file\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Extract the data from the XML elements\n",
    "\n",
    "    if cols: \n",
    "        rows = [{var: row.get(var) for var in cols} \n",
    "                 for row in root.findall('row')]\n",
    "    else: \n",
    "        rows = [{attr: row.get(attr) for attr in row.attrib} \n",
    "                for row in root.findall('row')]\n",
    "\n",
    "    # Create a DataFrame from the rows\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def is_numeric(series):\n",
    "    try:\n",
    "        pd.to_numeric(series, errors='raise')\n",
    "        return True\n",
    "    except (ValueError, TypeError):\n",
    "        return False\n",
    "    except pd.errors.OutOfBoundsDatetime:\n",
    "        return False\n",
    "\n",
    "def convertColumns(df: pd.DataFrame, \n",
    "                   exclude_id_cols = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Formatter function to change column types to more meaningful types than the default `str` that comes from the XML parsing\n",
    "\n",
    "    Returns: \n",
    "    pd.DataFrame: A DataFrame with the typed columns \n",
    "    \"\"\"\n",
    "\n",
    "    cols = df.columns\n",
    "    \n",
    "    if exclude_id_cols:\n",
    "        cols = [c for c in cols if not c.endswith(\"Id\")]\n",
    "    \n",
    "    for col in cols: \n",
    "        if col.endswith('Date'):\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')        \n",
    "        elif is_numeric(df[col]):\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    return df\n",
    "\n",
    "def parseHTML(text: str, col_name: str, **kwargs) -> dict:\n",
    "    \"\"\"\n",
    "    Parser for HTML comment and returns a tuple with comment text (or None) and list of links (or None)\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary with data of form {\"cleantext\": str, \"links\": list}.\n",
    "    \"\"\"\n",
    "\n",
    "    ret_dict = {}\n",
    "\n",
    "    #It gets moody because some comments are very terse.\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\") \n",
    "        if type(text) == float:\n",
    "            if 'bodytext' in kwargs and kwargs['bodytext']: \n",
    "                ret_dict[f'{col_name}Text'] = pd.NA\n",
    "            if 'links' in kwargs and kwargs['links']: \n",
    "                ret_dict[f'{col_name}URLs'] = pd.NA\n",
    "        else:\n",
    "            try: \n",
    "                soup = bs4.BeautifulSoup(text, 'lxml')\n",
    "                if 'bodytext' in kwargs and kwargs['bodytext']:\n",
    "                    ret_dict[f'{col_name}Text'] = soup.text.replace(\"\\n\",\" \")\n",
    "\n",
    "                if 'links' in kwargs and kwargs['links']:\n",
    "                    ret_dict[f'{col_name}URLs'] = [x['href'] for x in soup.find_all('a')\n",
    "                        if 'href' in x.attrs and \"://\" in x.get('href')]\n",
    "\n",
    "            except Exception as e:\n",
    "                print(type(e), e)\n",
    "\n",
    "    return ret_dict\n",
    "\n",
    "def cleanBody(body_series: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Worker that manages the pooled processing of many Body rows to clean.\n",
    "    \"\"\"\n",
    "    col_name = body_series.name\n",
    "\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        # Use apply to process each row in parallel\n",
    "        results = body_series.map(\n",
    "            lambda row: parseHTML(\n",
    "                row,bodytext=True,links=True,col_name=col_name))\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        results.tolist(), columns=[f'{col_name}Text', f'{col_name}URLs'])\n",
    "\n",
    "def splitTags(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Formatter that parses text of form `<tag1><tag2><...>` to return a list of `['tag1', 'tag2',...]`\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A two-column DataFrame with the same ID as original for merging.\n",
    "    \"\"\"\n",
    "\n",
    "    if type(text) != str or len(text) == 0:\n",
    "        return []\n",
    "    else:\n",
    "        return text[1:-1].split(\"><\")\n",
    "\n",
    "def cleanTags(tag_text_series: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Worker that manages the pooled processing of many Tags rows to clean.\n",
    "    \"\"\"\n",
    "\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        results = tag_text_series.map(splitTags)\n",
    "    \n",
    "    new_df = pd.DataFrame(results)\n",
    "    new_df.columns = ['TagsList']\n",
    "\n",
    "    return new_df\n",
    "\n",
    "def df_to_file(df, xml_path, file_type=\"parquet\", **kwargs):\n",
    "\n",
    "    msg = \"\"\n",
    "    \n",
    "    if file_type == \"parquet\": \n",
    "        import pyarrow.parquet as pq\n",
    "        import pyarrow as pa\n",
    "        \n",
    "        out_path = xml_path.parent / f\"{xml_path.stem}.{file_type}\"\n",
    "        \n",
    "        pq.write_table(pa.Table.from_pandas(df),out_path)\n",
    "        msg += f\"The file is available here: <br> `{str(out_path)}`\"\n",
    "        msg +=  \"<br> You can load this file and call `df` with <br> `df = \"\n",
    "        msg += f\" pq.read_table('{str(out_path)}').to_pandas()`<br>\"\n",
    "\n",
    "    \n",
    "    elif file_type == \"feather\": \n",
    "        out_path = xml_path.parent / f\"{xml_path.stem}.{file_type}\"\n",
    "        df.to_feather(out_path)\n",
    "        msg += f\"The file is available here: <br> `{str(out_path)}`\" \n",
    "        msg += f\"<br> You can load this file and call `df` \"\n",
    "        msg += f\"with <br> `df = pd.read_feather('{str(out_path)}')`\"\n",
    "          \n",
    "    elif file_type == \"pickle\":\n",
    "        import pickle\n",
    "        out_path = xml_path.parent / f\"{xml_path.stem}.{file_type}\"\n",
    "        df.to_pickle(out_path)\n",
    "        msg += f\"The file is available here: <br> `{str(out_path)}`\"\n",
    "        msg += f\"<br> You can load this file and call `df` \"\n",
    "        msg += f\"with <br> `df = pd.read_pickle('{str(out_path)}')`<br>\"\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return msg\n",
    "    \n",
    "def makeDataFrame(xml_path, file_out=\"feather\", **kwargs):\n",
    "    \"\"\"\n",
    "    Worker that steps through key processing from XML to DataFrame. \n",
    "\n",
    "    Parameters:\n",
    "        extract_body_text (Boolean): \n",
    "            Creates new columns for body text, the first has no HTML, \n",
    "            the second is a list of links in body text.\n",
    "        extract_tags (Boolean): Creates a new column for tags as a list \n",
    "            rather than a string of the form <tag1><tag2>\n",
    "        file_out (str): Returns the DataFrame in this format of \"pickle\", \n",
    "            \"feather\", \"csv\". Default is feather. CSV is not implemented yet.\n",
    "    \n",
    "    Returns:\n",
    "        str: A file path to the DataFrame that has been saved.\n",
    "    \"\"\"\n",
    "\n",
    "    df = convertXMLtoDataFrame(xml_path)\n",
    "    df = convertColumns(df)\n",
    "    concat_cols = [df]\n",
    "\n",
    "    if 'extract_body_text' in kwargs:\n",
    "        cols = kwargs['extract_body_text']\n",
    "        if type(cols) == str: \n",
    "            cols = [cols]\n",
    "            \n",
    "        if type(cols) == list:\n",
    "            for col in cols: \n",
    "                concat_cols.append(cleanBody(df[col]))\n",
    "                \n",
    "    if 'extract_tags' in kwargs and kwargs['extract_tags']:\n",
    "        concat_cols.append(cleanTags(df['Tags']))\n",
    "    \n",
    "    df = pd.concat(\n",
    "        concat_cols,\n",
    "        axis = 1)\n",
    "\n",
    "    return df_to_file(df,xml_path,file_type=file_out)\n",
    "\n",
    "def download_stackexchange_archive():\n",
    "    \"\"\"\n",
    "    Main worker function. Handles the downloading and population of the initial list, displaying options, and reporting outcome to user. \n",
    "    \"\"\"\n",
    "\n",
    "    se_intro = iw.Output()\n",
    "    display(se_intro)\n",
    "\n",
    "    with se_intro:\n",
    "        display(Markdown(\"Loading Stack Exchange list...\"))\n",
    "\n",
    "    df = process_archive_table(download_table(URL))\n",
    "\n",
    "    se_dropdown = iw.Dropdown(\n",
    "        options= [f\"{x[0]}. {x[1]} - {x[2]}\" \n",
    "                  for x in zip(df.index, df[\"stack_name\"], df[\"Size\"])],\n",
    "        value=None, \n",
    "        description=\"Archive:\")\n",
    "\n",
    "    se_dropdown.layout.width = '50%'\n",
    "\n",
    "    se_intro.clear_output()\n",
    "    with se_intro:\n",
    "        display(Markdown(\"Please select a Stack Exchange to process (the book uses 'movies')\"))\n",
    "\n",
    "    se_progress = iw.Output()\n",
    "\n",
    "    se_download_path = iw.Text(\n",
    "        description='Folder:',\n",
    "        value=str(DOWNLOAD_FOLDER))\n",
    "    \n",
    "    se_download_path.layout.width = '50%'\n",
    "\n",
    "    se_download = iw.Button(description=\"Download\", disabled=True)\n",
    "    \n",
    "    se_check_keep7z = iw.Checkbox(\n",
    "        description='Keep downloaded 7z file on disk', value=True) \n",
    "    se_check_useXML = iw.Checkbox(\n",
    "        description='Use existing XML file if found', value=True) \n",
    "    se_process_posts = iw.Checkbox(\n",
    "        description='Process Posts.xml to DataFrame', value=True) \n",
    "    se_process_users = iw.Checkbox(\n",
    "        description='Process Users.xml to DataFrame', value=False) \n",
    "    se_check_body_cols = iw.Checkbox(\n",
    "        description='Insert columns of text stripped of HTML and URLs',\n",
    "        value=True) \n",
    "    se_check_tags_list = iw.Checkbox(\n",
    "        description='Process Tags column to list of tags',\n",
    "        value=True) \n",
    "\n",
    "    se_file_type = iw.Dropdown(\n",
    "        options= [\"parquet\",\"feather\",\"pickle\"],\n",
    "        value=\"feather\", \n",
    "        description=\"File Type for Export:\")\n",
    "\n",
    "    se_file_type.layout.width = '50%'\n",
    "\n",
    "    \n",
    "    def handle_dropdown_change(change):\n",
    "        '''Internal worker to enable button state once archive selected'''\n",
    "\n",
    "        selected_option = change.new\n",
    "\n",
    "        url_index = int(selected_option.split(\".\")[0])\n",
    "\n",
    "        se_download.disabled = False\n",
    "        se_progress.clear_output()\n",
    "\n",
    "        with se_progress: \n",
    "            display(Markdown(\n",
    "            f\"You have selected: `{ df.loc[url_index]['stack_file_name']}`.\" +\n",
    "            f\"\\nThis option is `{df.loc[url_index]['Size']}` in size. \" +\n",
    "             \"Click to download.\"))\n",
    "\n",
    "    def handle_filetype_change(change):\n",
    "        se_download.disabled = False\n",
    "        se_progress.clear_output()\n",
    "        \n",
    "    def on_click_worker(_):\n",
    "        '''Internal worker to manage the process of downloading and processing\n",
    "        once a button has been pressed'''\n",
    "\n",
    "        se_download.disabled = True\n",
    "\n",
    "        with se_progress:\n",
    "\n",
    "            archive_name = df.loc[\n",
    "                int(se_dropdown.value.split(\".\")[0])][\"stack_file_name\"]\n",
    "            archive_remote_path = URL + '/' + archive_name\n",
    "            archive_dest = Path(se_download_path.value).resolve()\n",
    "            \n",
    "            dl_posts = se_process_posts.value\n",
    "            dl_users = se_process_users.value\n",
    "            \n",
    "            if se_check_useXML.value:\n",
    "                \n",
    "                archive_folder = archive_dest / archive_name.split(\".7z\")[0]\n",
    "                \n",
    "                if se_process_posts.value:\n",
    "                    if (archive_folder / \"Posts.xml\").exists():\n",
    "                        dl_posts = False\n",
    "                        display(Markdown(\"Using existing Posts XML file\"))\n",
    "                if se_process_users.value:\n",
    "                    if (archive_folder / \"Users.xml\").exists():\n",
    "                        dl_users = False\n",
    "                        display(Markdown(\"Using existing Users XML file\"))\n",
    "                    \n",
    "            if dl_posts or dl_users:\n",
    "                archive_folder = download_archive(\n",
    "                url = archive_remote_path, \n",
    "                dest = archive_dest, \n",
    "                output_widget = se_progress,\n",
    "                extract = True,\n",
    "                keep7z = se_check_keep7z.value)\n",
    "\n",
    "                display(Markdown(\"Processing downloaded file...\"))\n",
    "\n",
    "            status = \"\"\n",
    "        \n",
    "            if se_process_posts.value:\n",
    "                \n",
    "                body_cols = False\n",
    "                if se_check_body_cols.value:\n",
    "                    body_cols=\"Body\"\n",
    "\n",
    "                status += makeDataFrame(\n",
    "                    xml_path = archive_folder / (\"Posts.xml\"),\n",
    "                    file_out = se_file_type.value,\n",
    "                    extract_body_text = body_cols,\n",
    "                    extract_tags = se_check_tags_list.value)\n",
    "\n",
    "            if se_process_users.value:\n",
    "                body_cols = False\n",
    "                if se_check_body_cols.value:\n",
    "                    body_cols=\"AboutMe\" \n",
    "    \n",
    "                status += makeDataFrame(\n",
    "                    xml_path = archive_folder / (\"Users.xml\"),\n",
    "                    file_out = se_file_type.value,\n",
    "                    extract_body_text = body_cols)\n",
    "\n",
    "            display(Markdown(status))\n",
    "\n",
    "    settings_group = iw.VBox([\n",
    "        se_check_keep7z,\n",
    "        se_check_useXML,\n",
    "        se_check_body_cols,\n",
    "        se_check_tags_list,\n",
    "        se_process_posts,\n",
    "        se_process_users])\n",
    "\n",
    "    vbox = iw.VBox([\n",
    "        se_dropdown,\n",
    "        se_file_type,\n",
    "        se_download_path, \n",
    "        settings_group,\n",
    "        se_download, \n",
    "        se_progress])\n",
    "\n",
    "    se_dropdown.observe(handle_dropdown_change, names='value')\n",
    "    se_file_type.observe(handle_filetype_change)\n",
    "\n",
    "    \n",
    "    se_download.on_click(on_click_worker)\n",
    "\n",
    "    display(vbox)\n",
    "\n",
    "\n",
    "download_stackexchange_archive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016d82a0-ad33-4554-856e-0a04c3caabfb",
   "metadata": {},
   "source": [
    "## Change log \n",
    "\n",
    "## Update V1.2. October 19\n",
    "\n",
    "Better file management! If the folder with XML files has already been downloaded you can simply run download and it will process the files in there. This is handy if you want to add a Users.xml -> dataframe without redownloading everything. It is also handy if you want to have the files in a different format\n",
    "\n",
    "New format: parquet, default format remains `feather` as a faster i/o with higher compression and less import overhead. \n",
    "\n",
    "## Update V1.1. October 17 \n",
    "\n",
    "There is a breaking change in this code as I adopt a more consistent naming convention. `ListTags` is now `TagsLists` and `CleanBody` is now `BodyText` and `BodyURLs` in the case of Posts.xml. \n",
    "\n",
    "Version 1.1: This version now allows you to download either the `Posts.xml` or `Users.xml` which will be downloaded. The default export version is feather. Anaconda comes with the default packages but if you need to install, it would be `!pip install feather`. Parquet and pickle options are also available for backwards compatibility.\n",
    "\n",
    "Bug fixes: The export text now is quoted. The column types now enforce Id = Str and do not make int data as floats. The clean HTML code now also removes `\\n`. Code is a little more refactored in the XML parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb4004b-6107-4888-b0b9-93c23415e1e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
